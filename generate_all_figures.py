#!/usr/bin/env python
# coding: utf-8

# # Loading and Processing Data Generated by Simulation

# This script is for loading data generated in the FASRC cluster from simulations of my agent-based ant model, and then doing stuff with that data. Doing stuff life: 1) visualizing the data (ant trajectories) and 2) creating summary figures and analysis graphs.
# 
# There are plenty of parameter analysis tests. Here is a list of the basic ones:
# 
# - AD (Agent Density): varies WH/N, with 
#     - W=H=100 and 
#     - N ranging over [1, 10, 50, 100, 500, 1000]
# - AO (Agent Orientation): varies K_P vs theta_stochasticity, with 
#     - K_P in [0.1, 0.25, 0.5, 0.75, 1] 
#     - THETA_STOCH in [0, pi/16, pi/8, pi/4]
# - AS1 (Agent Speed, test 1): varies SMIN vs SMAX/SMIN, with 
#     - SMIN in [0, 2.5, 5, 7.5, 10]
#     - SMAX/SMIN in [1, 2, 4, 6, 8]
# - AS2 (Agent Speed, test 2): varies K_S vs SMAX/SMIN, with 
#     - K_S in [0.1, 0.25, 0.5, 0.75, 1]
#     - SMAX/SMIN in [1, 2, 4, 6, 8]
# - PE1 (Pheromone Evolution, test 1): varies D vs E, with
#     - D in [0.01, 0.05, 0.1, 0.2 ,0.4, 0.6, 0.8, 1]
#     - E in [0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5]
# - PE2 (Pheromone Evolution, test 2): varies D/E vs P_DROP, with
#     - D=0.6 and E in [0.02, 0.1, 0.2, 0.4, 0.6]
#     - P_DROP in [0.1, 0.25, 0.5, 0.75, 1]
# - PE3 (Pheromone Evolution, test 3): varies M vs P_DROP
#     - M in [1, 2.5, 5, 7.5, 10, 15]
#     - P_DROP in [0, 0.1, 0.25, 0.5, 0.75, 1]
# - PS (Pheromone Sensitivity): varies M vs MIN_PH_SENS, WITH
#     - M in [1, 2.5, 5, 10]
#     - MIN_PH_SENS in [0, 0.1, 0.2, 0.4]

import json
import numpy as np
from statistics import mean
from scipy.cluster.hierarchy import linkage,fcluster
from scipy.spatial import ConvexHull
import matplotlib.pyplot as plt

## Defining the Summary Parameter
def summary_analysis_function(data_trial):
    order_parameters = []
    for rep, trajectories in data_trial.items():
        count = 0
        for frame in trajectories:
            if count % 1 == 0:
                order_parameters.append(calculate_cluster_area_order(frame, 100*100, 4))
            count += 1
    time = np.arange(len(order_parameters))
    smoothed_time, smoothed_parameters = smooth(time,order_parameters,50)
    return (mean(smoothed_parameters[-50:]) - mean(smoothed_parameters[0:5])) / mean(smoothed_parameters[0:5]) #(smoothed_parameters[-1] - smoothed_parameters[0]) / smoothed_parameters[0]

def calculate_cluster_area_order(centroids, frame_area, num_clusters = 9):
    X = np.array(centroids)
    Z = linkage(X,'centroid')
    cluster_assignments = fcluster(Z,num_clusters,'maxclust')
    
    total_area = 0
    for i in range(max(cluster_assignments)):
        idxs = [idx for idx,x in enumerate(cluster_assignments) if x == i+1]
        cluster_points = [p for idx,p in enumerate(X) if idx in idxs]
        if(len(cluster_points) < 3):
            continue
        try:
            total_area += (ConvexHull(cluster_points)).volume
        except:
            continue
    
    return total_area/frame_area

def smooth(time,data,window_size):
    data_s = np.convolve(data, np.ones(window_size)/window_size, mode='valid')
    time_s = time[(window_size-1)//2 : -(window_size-1)//2]
    return time_s,data_s


data_files = ["data_AO/data_AO_5x5x1.txt", "data_AS1/data_AS1_5x6x1.txt", 
                "data_AS2/data_AS2_5x6x1.txt", "data_PE1/data_PE1_7x8x1.txt", "data_PE2/data_PE2_6x5x1.txt", 
                "data_PE3/data_PE3_6x6x1.txt", "data_PS/data_PS_6x5x1.txt"]
analysis_titles = ["AO", "AS1", "AS2", "PE1", "PE2", "PE3", "PS"]
analysis_axes = [("K_P", "T_STOCH"), ("S_MIN", "S_RATIO"), ("K_S", "S_MAX (S_MIN is Constant 3)"), ("D", "E"), 
                ("P_DROP", "E (D is Constant 0.6)"), ("M", "P_DROP"), ("M", "MIN_PH_SENS")]

for idx, f in enumerate(data_files):
    ## Data File
    data_file = f

    ## Load Data
    with open(data_file, 'r') as f:
        # Read the first line which contains the dictionary
        dictionary_str = f.readline().strip()

        # Convert the string representation of dictionary to a Python dictionary
        data = json.loads(dictionary_str)

    ## Create Summary Parameter Array
    summary_param = {}
    for A_k,A_t in data.items():
        summary_param[float(A_k)] = {}
        for B_k,B_t in A_t.items():
            summary_param[float(A_k)][float(B_k)] = summary_analysis_function(B_t)

    ## Draw Graph of Summary Parameter
    summary_analysis = summary_param

    # Extract parameters and summary parameters
    params_A = sorted(summary_analysis.keys())  # Sort the keys
    params_B = sorted(summary_analysis[params_A[0]].keys())  # Sort the keys
    summary_parameters = [[summary_analysis[A][B] for B in params_B] for A in params_A]

    # Determine the overall minimum and maximum values of summary parameters
    min_value = min(min(row) for row in summary_parameters)
    max_value = max(max(row) for row in summary_parameters)

    # Create a colormap
    cmap = plt.get_cmap('viridis')  # You can choose any colormap you like

    # Create meshgrid of parameter values
    A_values, B_values = np.meshgrid(params_A, params_B)

    # Create 2D grid of color values and transpose it.
    color_values_grid = np.array(summary_parameters)
    color_values_grid = np.transpose(color_values_grid)

    # Plot the data
    fig, ax = plt.subplots()
    color_map = ax.imshow(color_values_grid, extent=[min(params_A), max(params_A), min(params_B), max(params_B)], cmap='viridis', aspect='auto', origin='lower')

    # Customize the plot
    ax.set_xlabel(analysis_axes[idx][0])
    ax.set_ylabel(analysis_axes[idx][1])
    ax.set_title('Summary Analysis' + analysis_titles[idx])

    # Show colorbar
    plt.colorbar(color_map, ax=ax)

    plt.savefig("figures/figure_" + analysis_titles[idx] + ".jpg")

###
